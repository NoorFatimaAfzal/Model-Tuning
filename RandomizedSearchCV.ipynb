{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RandomizedSearchCV in Scikit-Learn\n",
        "\n",
        "## What is RandomizedSearchCV?\n",
        "\n",
        "`RandomizedSearchCV` is a hyperparameter tuning method in Scikit-learn. It randomly samples a fixed number of hyperparameter combinations from a specified search space and evaluates them using cross-validation. This method allows you to search for the best hyperparameters for a model in a more efficient way compared to exhaustive methods like `GridSearchCV`.\n",
        "\n",
        "## Why Use RandomizedSearchCV?\n",
        "\n",
        "- **Efficiency**: Instead of evaluating all possible hyperparameter combinations (as in GridSearchCV), it randomly selects a subset to evaluate, saving computational time and resources.\n",
        "- **Flexibility**: You can control the number of iterations (hyperparameter combinations) it evaluates by setting the `n_iter` parameter.\n",
        "- **Exploration**: It explores a broader range of hyperparameter values than GridSearchCV since it uses random sampling.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **param_distributions**: A dictionary where each key is a hyperparameter name and its value is a distribution or a list of possible values.\n",
        "- **n_iter**: The number of parameter settings that are sampled and evaluated. The more iterations, the broader the search, but it will take more computation time.\n",
        "- **cv**: The number of folds used in cross-validation to evaluate model performance.\n",
        "- **scoring**: The evaluation metric used to determine the best hyperparameter combination (e.g., accuracy, mean squared error).\n",
        "- **random_state**: Ensures reproducibility by controlling the randomness in the parameter selection.\n",
        "\n",
        "## Differences Between RandomizedSearchCV and GridSearchCV\n",
        "\n",
        "| Feature                | RandomizedSearchCV                       | GridSearchCV                       |\n",
        "|------------------------|------------------------------------------|------------------------------------|\n",
        "| Search Method           | Random sampling from hyperparameter space | Exhaustive search over all combinations |\n",
        "| Computation Time        | Generally faster                        | Generally slower                   |\n",
        "| Number of Combinations  | User-specified (via `n_iter`)            | All possible combinations evaluated |\n",
        "| Flexibility             | Can handle continuous distributions      | Limited to fixed lists of values   |\n",
        "| Thoroughness            | Less thorough but efficient              | Thorough but computationally expensive |\n",
        "\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Parameter Space**: Define the hyperparameter space with distributions of possible values (either continuous or discrete).\n",
        "2. **Random Sampling**: Randomly sample combinations from the parameter space.\n",
        "3. **Cross-Validation**: For each sampled combination, perform cross-validation to evaluate model performance.\n",
        "4. **Best Parameters**: Identify the combination of hyperparameters that yields the best performance."
      ],
      "metadata": {
        "id": "84GZSMifJoIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomForestClassifier**"
      ],
      "metadata": {
        "id": "xFmKdE4-KQrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQva9wK9Jf9_",
        "outputId": "d4e0dfd7-9e16-457a-91fd-b1e4c93bdd69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 142}\n",
            "Best score found:  0.9428571428571428\n",
            "Test set accuracy:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter distributions for RandomForestClassifier\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),  # Randomly choose between 50 and 200 estimators\n",
        "    'max_depth': [None, 10, 20, 30],   # List of possible max_depth values\n",
        "    'min_samples_split': randint(2, 11) # Randomly choose min_samples_split between 2 and 10\n",
        "}\n",
        "\n",
        "# Create a RandomForestClassifier model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', random_state=42, verbose=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best score found: \", random_search.best_score_)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVC**"
      ],
      "metadata": {
        "id": "kqLLMVxzLCEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter distributions for SVC\n",
        "param_dist = {\n",
        "    'C': uniform(0.1, 10),  # Continuous distribution of regularization parameter 'C'\n",
        "    'kernel': ['linear', 'rbf', 'poly'],  # Different kernel types\n",
        "    'gamma': uniform(0.001, 0.1)  # Kernel coefficient for 'rbf', 'poly' kernels\n",
        "}\n",
        "\n",
        "# Create a Support Vector Classifier model\n",
        "model = SVC(random_state=42)\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', random_state=42, verbose=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best score found: \", random_search.best_score_)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgatpeuKKLUE",
        "outputId": "8c05ff5a-bb31-4b7f-9b57-1c6a21b06e5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'C': 2.2233911067827616, 'gamma': 0.019182496720710065, 'kernel': 'linear'}\n",
            "Best score found:  0.9714285714285715\n",
            "Test set accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KNeighborsClassifier**"
      ],
      "metadata": {
        "id": "DsK-npy-K-0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter distributions for KNeighborsClassifier\n",
        "param_dist = {\n",
        "    'n_neighbors': randint(1, 31),  # Randomly choose the number of neighbors between 1 and 30\n",
        "    'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metric\n",
        "}\n",
        "\n",
        "# Create a KNeighborsClassifier model\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', random_state=42, verbose=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best score found: \", random_search.best_score_)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38l5JIs9KoJ9",
        "outputId": "a3df9106-a37a-4c95-a743-abdd662e50ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'metric': 'euclidean', 'n_neighbors': 1, 'weights': 'distance'}\n",
            "Best score found:  0.9523809523809523\n",
            "Test set accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GradientBoostingClassifier**"
      ],
      "metadata": {
        "id": "UjYzor5AK7ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter distributions for GradientBoostingClassifier\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),         # Number of boosting stages to be run\n",
        "    'learning_rate': uniform(0.01, 0.3),      # Step size at each iteration\n",
        "    'max_depth': randint(3, 10),              # Maximum depth of the individual trees\n",
        "    'subsample': uniform(0.7, 0.3)            # Fraction of samples used for fitting each base learner\n",
        "}\n",
        "\n",
        "# Create a GradientBoostingClassifier model\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', random_state=42, verbose=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best score found: \", random_search.best_score_)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leptXuqlKxtG",
        "outputId": "14ed89ef-60f2-46e9-8e11-9c61bb95d585"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'learning_rate': 0.19355586841671385, 'max_depth': 4, 'n_estimators': 64, 'subsample': 0.8368209952651107}\n",
            "Best score found:  0.9428571428571428\n",
            "Test set accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LogisticRegression**"
      ],
      "metadata": {
        "id": "KV4ylpNRMOiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter distributions for LogisticRegression with compatible combinations\n",
        "param_dist = {\n",
        "    'C': uniform(0.001, 10),  # Inverse of regularization strength\n",
        "    'penalty': ['l2', 'none'],  # Type of regularization, compatible with most solvers\n",
        "    'solver': ['newton-cg', 'lbfgs', 'saga'],  # Solvers compatible with 'l2' or 'none'\n",
        "    'max_iter': randint(50, 300)  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Create a LogisticRegression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='accuracy', random_state=42, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best score found: \", random_search.best_score_)\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test set accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiCN4P_9K4or",
        "outputId": "152eeab3-2fab-45af-f947-3a8901105131"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'C': 3.746401188473625, 'max_iter': 142, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Best score found:  0.9619047619047618\n",
            "Test set accuracy:  1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ridge**"
      ],
      "metadata": {
        "id": "xcackj97My5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the California housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter distributions for Ridge regression\n",
        "param_dist = {\n",
        "    'alpha': uniform(0.001, 100),  # Regularization strength\n",
        "    'fit_intercept': [True, False]  # Whether to include an intercept term\n",
        "}\n",
        "\n",
        "# Create a Ridge regression model\n",
        "model = Ridge(random_state=42)\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5,\n",
        "                                   scoring='neg_mean_squared_error', random_state=42, verbose=1)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best score found: \", -random_search.best_score_)  # Negate to get positive MSE\n",
        "\n",
        "# Use the best model to make predictions on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance on the test set\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test set Mean Squared Error: \", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q2-RShSLTOY",
        "outputId": "d6058999-7c99-4e9b-fda8-5dacef7caa30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best parameters found:  {'alpha': 37.455011884736244, 'fit_intercept': True}\n",
            "Best score found:  0.527062792279338\n",
            "Test set Mean Squared Error:  0.5286047409176791\n"
          ]
        }
      ]
    }
  ]
}